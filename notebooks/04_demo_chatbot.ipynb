{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c8fc959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "RESULTS_DIR = Path(\"../results\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50e64f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents and FAISS index loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load document metadata\n",
    "df = pd.read_csv(RESULTS_DIR / \"retriever_documents.csv\")\n",
    "\n",
    "# Load prebuilt IVF FAISS index\n",
    "index_path = RESULTS_DIR / \"faiss_index_ivf.bin\"\n",
    "index = faiss.read_index(str(index_path))\n",
    "index.nprobe = 10  # tune for speed vs accuracy\n",
    "print(\"Documents and FAISS index loaded.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0150d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Embedding model on GPU if possible\n",
    "try:\n",
    "    embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cuda\")\n",
    "except RuntimeError:\n",
    "    print(\"Insufficient GPU memory. Using CPU for embedder.\")\n",
    "    embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\")\n",
    "\n",
    "# Reranker on CPU to save GPU memory\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\", device=\"cpu\")\n",
    "\n",
    "# Generator model (text-generation) on GPU if available\n",
    "device_id = 0 if torch.cuda.is_available() else -1\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\", device=device_id)\n",
    "\n",
    "print(\"Models loaded successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b1ff9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chat(query, top_k=3):\n",
    "    # Encode query\n",
    "    query_emb = embedder.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # Retrieve top_k documents from FAISS\n",
    "    distances, indices = index.search(query_emb, top_k)\n",
    "    retrieved_docs = df.iloc[indices[0]]\n",
    "    \n",
    "    # Optional reranking (on CPU)\n",
    "    pairs = [[query, doc] for doc in retrieved_docs[\"text\"].tolist()]\n",
    "    scores = reranker.predict(pairs)\n",
    "    reranked = sorted(zip(retrieved_docs[\"text\"], scores), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Prepare context for generation\n",
    "    context = \"\\n\".join([doc for doc, _ in reranked[:top_k]])\n",
    "    input_text = f\"Answer the question using the following context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "    # Generate answer\n",
    "    response = generator(input_text, max_length=200, num_return_sequences=1)[0][\"generated_text\"]\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "821717d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question using the following context:\n",
      "A data set is a collection of information organized as a stream of bytes in logical record and block structures for use by IBM mainframe operating systems. The record format is determined by data set organization, record format and other parameters. The physical structure of each record is nearly the same, and uniform throughout a data set. This is specified in the data control block record format parameter.\n",
      "The data set lists values for each of the variables, such as height and weight of an object, for each member of the data set. Each value is known as a datum. The data set may comprise data for one or more members, corresponding to the number of rows. The term data set may also be used more loosely, to refer to the data in a collection of closely related tables, corresponding to a particular experiment or event. An example of this type is the data sets collected by space agencies performing experiments with instruments aboard space probes.\n",
      "Most commonly a data set corresponds to the contents of a single database table, or a single statistical data matrix, where every column of the table represents a particular variable, and each row corresponds to a given member of the data set in question.\n",
      "\n",
      "Question: What is in the dataset?\n",
      "Answer: is used in computing computing devices, such as computer computers, mobile phones and other devices that are connected to the Internet. The data set is a physical record of data set organization. The data set is represented as a block structure. The record structure is a set of physical data fields, representing the data of the data set. The physical record of data set organization is defined by the data structure of the block structure.\n",
      "The logical record of data set organization is a block structure. The block structure is a collection of logical record and block structures for use by IBM mainframe operating systems. The record format is determined by data set organization, record format and other parameters. The physical structure of each record is nearly the same, and uniform throughout a data set. This is specified in the data control block record format parameter. The data set lists values for each of the variables, such as height and weight of an object, for each member of the data set. Each value is known as a datum. The data set may comprise data for one or more members, corresponding to the number of rows. The term data set may also be used more loosely, to refer to the data in a collection of closely related tables, corresponding to a particular experiment or event. An example of this type\n"
     ]
    }
   ],
   "source": [
    "response = rag_chat(\"What is in the dataset?\")\n",
    "print(response)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
